{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03eb5143-da5f-4f2b-9e04-af6ed98ef52f",
   "metadata": {},
   "source": [
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3f4dae1-1ce8-4b5c-820b-f290c0468b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "665f1245-c06d-4a34-b62d-4dda7648c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"My app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037f2222-e215-4eb8-84de-af334469e660",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"retail-data/all/online-retail-dataset.csv\")\\\n",
    "    .coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42f2839e-fb36-4735-aa6a-245a310152f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: string, UnitPrice: double, CustomerID: int, Country: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3527bf1-88b5-4020-a9e7-8e304d28d34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0b81f17-4379-4815-9e3f-4b944bea89a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e229f93c-af8c-4f63-87a1-f2fc07edb615",
   "metadata": {},
   "source": [
    "### count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7722eef7-c9a3-4ea6-8655-74d8bab232a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(StockCode)=541909)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).collect() #count as transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e757cae-d4d4-454c-b045-bd4df7fd4f48",
   "metadata": {},
   "source": [
    "### Count Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a6ad7c2-60e2-423f-93fb-fbc9fb617297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(DISTINCT StockCode)=4070)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914f0c7-139e-4b41-a89a-14563bf2fecb",
   "metadata": {},
   "source": [
    "### Approximate Count Distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b656ac5-3353-4223-8774-ed48d36505c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(approx_count_distinct(StockCode)=3364)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\",0.1)).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b3b74-5742-4731-8552-8acf3bceea31",
   "metadata": {},
   "source": [
    "### First ans Last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "083e8636-8a48-493f-b980-789238f83925",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(first(StockCode)='85123A', last(StockCode)='22138')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import first,last\n",
    "df.select(first(\"StockCode\"),last(\"StockCode\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0c5d4f-3460-4ea1-a4b9-d75f59d8489d",
   "metadata": {},
   "source": [
    "### Min and Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c124a72-5b9a-4323-82cb-150b17291e5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(min(Quantity)=-80995, max(Quantity)=80995)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import min,max\n",
    "df.select(min(\"Quantity\"),max(\"Quantity\")).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d1938f-60fb-4db5-b52c-13d752eaac86",
   "metadata": {},
   "source": [
    "### Sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c35d4be-6322-4181-afe2-8c8e31002caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32add8b-4e5d-491a-8c9f-6cf3a29adc7e",
   "metadata": {},
   "source": [
    "### sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a18a0f54-d27e-4f15-9b71-ef4094e3b917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa411a7-4171-4276-92d7-778d8644c823",
   "metadata": {},
   "source": [
    "### Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64420ff3-f1f9-4234-aebc-89e9a09fdb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row((total_purchases / total_transactions)=9.55224954743324, avg_purchases=9.55224954743324, mean_purchases=9.55224954743324)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum,count,avg,expr\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    "    .selectExpr(\n",
    "        \"total_purchases/total_transactions\",\n",
    "        \"avg_purchases\",\n",
    "        \"mean_purchases\")\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deca15c-73c6-444b-a91c-dfa3edc40d3a",
   "metadata": {},
   "source": [
    "### Variance and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d9bd439-e33c-4278-af5e-7431a6734177",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(var_pop(Quantity)=47559.303646609056, var_samp(Quantity)=47559.391409298754, stddev_pop(Quantity)=218.08095663447796, stddev_samp(Quantity)=218.08115785023418)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import var_pop,stddev_pop\n",
    "from pyspark.sql.functions import var_samp,stddev_samp\n",
    "\n",
    "df.select(\n",
    "    var_pop(\"Quantity\"),\n",
    "    var_samp(\"Quantity\"),\n",
    "    stddev_pop(\"Quantity\"),\n",
    "    stddev_samp(\"Quantity\"))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ab4267-9b95-46a1-9b2e-970f9eae1ad2",
   "metadata": {},
   "source": [
    "### Skewness and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8625323a-fa43-4706-83cc-5274ddd94fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(skewness(Quantity)=-0.2640755761052562, kurtosis(Quantity)=119768.05495536952)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "df.select(\n",
    "    skewness(\"Quantity\"),\n",
    "    kurtosis(\"Quantity\"))\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2b897d-0389-439b-80fa-61d254f987c0",
   "metadata": {},
   "source": [
    "### Covariance and Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cfea3d3-b594-46d7-8c34-056e3133cfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "|     4.912186085635685E-4|             1052.7280543902734|            1052.7260778741693|\n",
      "+-------------------------+-------------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import corr,covar_pop,covar_samp\n",
    "\n",
    "df.select(\n",
    "    corr(\"InvoiceNo\",\"Quantity\"),\n",
    "    covar_samp(\"InvoiceNo\",\"Quantity\"),\n",
    "    covar_pop(\"InvoiceNo\",\"Quantity\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516fe6f2-cd27-4c0b-9fb0-360019bb75d8",
   "metadata": {},
   "source": [
    "### Aggregating to Complext Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fc7b3a5-a982-4707-a373-ae6dc78913a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import collect_set,collect_list\n",
    "\n",
    "df.agg(\n",
    "    collect_set(\"Country\"),\n",
    "    collect_list(\"Country\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1e7bf1-69b4-48b9-802e-f55c0cf453d9",
   "metadata": {},
   "source": [
    "### Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a564ee3-8a4c-4408-8185-3fb5c2e4dbdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n",
      "|invoiceNo|count|\n",
      "+---------+-----+\n",
      "|   536596|    6|\n",
      "|   536938|   14|\n",
      "|   537252|    1|\n",
      "|   537691|   20|\n",
      "|   538041|    1|\n",
      "|   538184|   26|\n",
      "|   538517|   53|\n",
      "|   538879|   19|\n",
      "|   539275|    6|\n",
      "|   539630|   12|\n",
      "|   540499|   24|\n",
      "|   540540|   22|\n",
      "|  C540850|    1|\n",
      "|   540976|   48|\n",
      "|   541432|    4|\n",
      "|   541518|  101|\n",
      "|   541783|   35|\n",
      "|   542026|    9|\n",
      "|   542375|    6|\n",
      "|   536597|   28|\n",
      "+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"invoiceNo\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "82bb7840-ab16-4e3e-b0de-b59d861fb90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n",
      "|InvoiceNo|CustomerId|count|\n",
      "+---------+----------+-----+\n",
      "|   536846|     14573|   76|\n",
      "|   537026|     12395|   12|\n",
      "|   537883|     14437|    5|\n",
      "|   538068|     17978|   12|\n",
      "|   538279|     14952|    7|\n",
      "|   538800|     16458|   10|\n",
      "|   538942|     17346|   12|\n",
      "|  C539947|     13854|    1|\n",
      "|   540096|     13253|   16|\n",
      "|   540530|     14755|   27|\n",
      "|   541225|     14099|   19|\n",
      "|   541978|     13551|    4|\n",
      "|   542093|     17677|   16|\n",
      "|   536596|      NULL|    6|\n",
      "|   537252|      NULL|    1|\n",
      "|   538041|      NULL|    1|\n",
      "|   537159|     14527|   28|\n",
      "|   537213|     12748|    6|\n",
      "|   538191|     15061|   16|\n",
      "|  C539301|     13496|    1|\n",
      "+---------+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\",\"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18cb7b-e2d6-49db-a875-b6cf89d0955f",
   "metadata": {},
   "source": [
    "### Grouping with expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fbc4dc78-9856-48fa-8943-6086d894ab98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n",
      "|InvoiceNo|quan|count(Quantity)|\n",
      "+---------+----+---------------+\n",
      "|   536596|   6|              6|\n",
      "|   536938|  14|             14|\n",
      "|   537252|   1|              1|\n",
      "|   537691|  20|             20|\n",
      "|   538041|   1|              1|\n",
      "|   538184|  26|             26|\n",
      "|   538517|  53|             53|\n",
      "|   538879|  19|             19|\n",
      "|   539275|   6|              6|\n",
      "|   539630|  12|             12|\n",
      "|   540499|  24|             24|\n",
      "|   540540|  22|             22|\n",
      "|  C540850|   1|              1|\n",
      "|   540976|  48|             48|\n",
      "|   541432|   4|              4|\n",
      "|   541518| 101|            101|\n",
      "|   541783|  35|             35|\n",
      "|   542026|   9|              9|\n",
      "|   542375|   6|              6|\n",
      "|   536597|  28|             28|\n",
      "+---------+----+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\")\\\n",
    "    .agg(\n",
    "        count(\"Quantity\").alias(\"quan\"),\n",
    "        expr(\"count(Quantity)\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0beee472-3613-4222-8124-4f984bdbb320",
   "metadata": {},
   "source": [
    "### Grouping with Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5375ef56-16b0-4712-9c76-e1ae282e197f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------+--------------------+\n",
      "|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n",
      "+---------+------------------+--------------------+\n",
      "|   536596|               1.5|  1.1180339887498947|\n",
      "|   536938|33.142857142857146|  20.698023172885524|\n",
      "|   537252|              31.0|                 0.0|\n",
      "|   537691|              8.15|   5.597097462078001|\n",
      "|   538041|              30.0|                 0.0|\n",
      "|   538184|12.076923076923077|   8.142590198943392|\n",
      "|   538517|3.0377358490566038|  2.3946659604837897|\n",
      "|   538879|21.157894736842106|  11.811070444356483|\n",
      "|   539275|              26.0|  12.806248474865697|\n",
      "|   539630|20.333333333333332|  10.225241100118645|\n",
      "|   540499|              3.75|  2.6653642652865788|\n",
      "|   540540|2.1363636363636362|  1.0572457590557278|\n",
      "|  C540850|              -1.0|                 0.0|\n",
      "|   540976|10.520833333333334|   6.496760677872902|\n",
      "|   541432|             12.25|  10.825317547305483|\n",
      "|   541518| 23.10891089108911|  20.550782784878713|\n",
      "|   541783|11.314285714285715|   8.467657556242811|\n",
      "|   542026| 7.666666666666667|   4.853406592853679|\n",
      "|   542375|               8.0|  3.4641016151377544|\n",
      "|   536597|2.5357142857142856|  2.7448932175059566|\n",
      "+---------+------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\")\\\n",
    "    .agg(expr(\"avg(Quantity)\"),\n",
    "         expr(\"stddev_pop(Quantity)\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c180fb-fcf2-4f5b-9d65-1227444b5827",
   "metadata": {},
   "source": [
    "### Window Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b7210e86-ad0d-491b-ae1b-6b4ea7018cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "dfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\n",
    "dfWithDate.createOrReplaceTempView(\"dfWithDate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c1349401-a257-4263-a216-2e595e7965a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc\n",
    "windowSpec = Window\\\n",
    ".partitionBy(\"CustomerId\", \"date\")\\\n",
    ".orderBy(desc(\"Quantity\"))\\\n",
    ".rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c70d6ea-5046-455b-8975-0cc4ce04a624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "maxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0ae304d0-185f-4763-917d-6b2fbd898517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|\n",
      "+----------+----------+--------+------------+-----------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|\n",
      "|     12347|2010-12-07|      36|           1|                1|\n",
      "|     12347|2010-12-07|      30|           2|                2|\n",
      "|     12347|2010-12-07|      24|           3|                3|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|      12|           4|                4|\n",
      "|     12347|2010-12-07|       6|          17|                5|\n",
      "|     12347|2010-12-07|       6|          17|                5|\n",
      "+----------+----------+--------+------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import dense_rank, rank\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)\n",
    "purchaseRank = rank()\\\n",
    " .over(windowSpec)\n",
    "from pyspark.sql.functions import col\n",
    "dfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    ".select(\n",
    "col(\"CustomerId\"),\n",
    "col(\"date\"),\n",
    "col(\"Quantity\"),\n",
    "purchaseRank.alias(\"quantityRank\"),\n",
    "purchaseDenseRank.alias(\"quantityDenseRank\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "47d64a80-9187-42aa-9f15-ff503f2db2c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|CustomerId|      date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "|     12346|2011-01-18|   74215|           1|                1|              74215|\n",
      "|     12346|2011-01-18|  -74215|           2|                2|              74215|\n",
      "|     12347|2010-12-07|      36|           1|                1|                 36|\n",
      "|     12347|2010-12-07|      30|           2|                2|                 36|\n",
      "|     12347|2010-12-07|      24|           3|                3|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|      12|           4|                4|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "|     12347|2010-12-07|       6|          17|                5|                 36|\n",
      "+----------+----------+--------+------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dfWithDate\\\n",
    "    .where(\"CustomerId IS NOT NULL\")\\\n",
    "    .orderBy(\"CustomerId\")\\\n",
    "    .select(\n",
    "        col(\"CustomerId\"),\n",
    "        col(\"date\"),\n",
    "        col(\"Quantity\"),\n",
    "        purchaseRank.alias(\"quantityRank\"),\n",
    "        purchaseDenseRank.alias(\"quantityDenseRank\"),\n",
    "        maxPurchaseQuantity.alias(\"maxPurchaseQuantity\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16385cd6-39df-44e0-a5d6-d40457ebc5c3",
   "metadata": {},
   "source": [
    "### Rollups\n",
    "A Rollup is a multi-dimensional aggregation that performs a variety of group by style calculations\n",
    "for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c254784d-b4b7-45d3-9e9e-8424ab7d00c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      NULL|          NULL|       5176450|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|United Kingdom|         23949|\n",
      "|2010-12-01|        France|           449|\n",
      "|2010-12-01|        Norway|          1852|\n",
      "|2010-12-01|          NULL|         26814|\n",
      "|2010-12-01|       Germany|           117|\n",
      "|2010-12-01|          EIRE|           243|\n",
      "|2010-12-01|   Netherlands|            97|\n",
      "|2010-12-02|          NULL|         21023|\n",
      "|2010-12-02|United Kingdom|         20873|\n",
      "|2010-12-02|       Germany|           146|\n",
      "|2010-12-02|          EIRE|             4|\n",
      "|2010-12-03|          NULL|         14830|\n",
      "|2010-12-03|      Portugal|            65|\n",
      "|2010-12-03|         Spain|           400|\n",
      "|2010-12-03|United Kingdom|         10439|\n",
      "|2010-12-03|   Switzerland|           110|\n",
      "|2010-12-03|       Germany|           170|\n",
      "|2010-12-03|          EIRE|          2575|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = dfWithDate.rollup(\"Date\",\"Country\")\\\n",
    "    .agg(sum(\"Quantity\"))\\\n",
    "    .selectExpr(\"Date\",\"Country\",\"`sum(Quantity)` as total_quantity\")\\\n",
    "    .orderBy(\"Date\")\n",
    "\n",
    "rolledUpDF.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "07b417b8-07b1-4d4a-a719-34eae2d192e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n",
      "|      Date|Country|total_quantity|\n",
      "+----------+-------+--------------+\n",
      "|      NULL|   NULL|       5176450|\n",
      "|2010-12-01|   NULL|         26814|\n",
      "|2010-12-02|   NULL|         21023|\n",
      "|2010-12-03|   NULL|         14830|\n",
      "|2010-12-05|   NULL|         16395|\n",
      "|2010-12-06|   NULL|         21419|\n",
      "|2010-12-07|   NULL|         24995|\n",
      "|2010-12-08|   NULL|         22741|\n",
      "|2010-12-09|   NULL|         18431|\n",
      "|2010-12-10|   NULL|         20297|\n",
      "|2010-12-12|   NULL|         10565|\n",
      "|2010-12-13|   NULL|         17623|\n",
      "|2010-12-14|   NULL|         20098|\n",
      "|2010-12-15|   NULL|         18229|\n",
      "|2010-12-16|   NULL|         29632|\n",
      "|2010-12-17|   NULL|         16069|\n",
      "|2010-12-19|   NULL|          3795|\n",
      "|2010-12-20|   NULL|         14965|\n",
      "|2010-12-21|   NULL|         15467|\n",
      "|2010-12-22|   NULL|          3192|\n",
      "+----------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c2e8dad-0c17-40ad-9959-3aa2fc8ca81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------------+\n",
      "|Date|Country|total_quantity|\n",
      "+----+-------+--------------+\n",
      "|NULL|   NULL|       5176450|\n",
      "+----+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Date IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34960bb6-df67-4644-85e6-714d75708cfc",
   "metadata": {},
   "source": [
    "### Cube\n",
    " A cube takes the rollup takes a rollup to a level deeper. Rather than treating\n",
    " things hierarchically a cube does the same thing across all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8dde8a0d-9c45-4ef3-bf87-b9f9e4e7b1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------+-------------+\n",
      "|Date|        Country|sum(Quantity)|\n",
      "+----+---------------+-------------+\n",
      "|NULL|         Cyprus|         6317|\n",
      "|NULL|        Belgium|        23152|\n",
      "|NULL|        Austria|         4827|\n",
      "|NULL|        Germany|       117448|\n",
      "|NULL|      Lithuania|          652|\n",
      "|NULL|         Poland|         3653|\n",
      "|NULL|        Iceland|         2458|\n",
      "|NULL|      Australia|        83653|\n",
      "|NULL|        Finland|        10666|\n",
      "|NULL|         Norway|        19247|\n",
      "|NULL|          Italy|         7999|\n",
      "|NULL|           EIRE|       142637|\n",
      "|NULL| United Kingdom|      4263829|\n",
      "|NULL|          Spain|        26824|\n",
      "|NULL|        Lebanon|          386|\n",
      "|NULL|        Bahrain|          260|\n",
      "|NULL|           NULL|      5176450|\n",
      "|NULL|         Israel|         4353|\n",
      "|NULL|Channel Islands|         9479|\n",
      "|NULL|    Switzerland|        30325|\n",
      "+----+---------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The grand total across all dates and countries\n",
    "# The grand total for each date across all countries\n",
    "# The grand total for each country on each date\n",
    "# The grand total for each country across all date\n",
    "dfWithDate.cube(\"Date\",\"Country\")\\\n",
    "    .agg(sum(col(\"Quantity\")))\\\n",
    "    .select(\"Date\",\"Country\",\"sum(Quantity)\")\\\n",
    "    .orderBy(\"Date\")\\\n",
    "    .show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea31082-cf48-4ece-81cc-fecc4c2ff05c",
   "metadata": {},
   "source": [
    "### Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "30f93913-a3a9-437b-adcf-55155bf69a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pivoted = dfWithDate\\\n",
    "    .groupBy(\"date\")\\\n",
    "    .pivot(\"Country\")\\\n",
    "    .agg({\"quantity\":\"sum\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "66154f9c-b59d-45ff-b842-17cc3e42edaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['date',\n",
       " 'Australia',\n",
       " 'Austria',\n",
       " 'Bahrain',\n",
       " 'Belgium',\n",
       " 'Brazil',\n",
       " 'Canada',\n",
       " 'Channel Islands',\n",
       " 'Cyprus',\n",
       " 'Czech Republic',\n",
       " 'Denmark',\n",
       " 'EIRE',\n",
       " 'European Community',\n",
       " 'Finland',\n",
       " 'France',\n",
       " 'Germany',\n",
       " 'Greece',\n",
       " 'Hong Kong',\n",
       " 'Iceland',\n",
       " 'Israel',\n",
       " 'Italy',\n",
       " 'Japan',\n",
       " 'Lebanon',\n",
       " 'Lithuania',\n",
       " 'Malta',\n",
       " 'Netherlands',\n",
       " 'Norway',\n",
       " 'Poland',\n",
       " 'Portugal',\n",
       " 'RSA',\n",
       " 'Saudi Arabia',\n",
       " 'Singapore',\n",
       " 'Spain',\n",
       " 'Sweden',\n",
       " 'Switzerland',\n",
       " 'USA',\n",
       " 'United Arab Emirates',\n",
       " 'United Kingdom',\n",
       " 'Unspecified']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivoted.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "241569ad-0d83-428f-a0fa-893b89c84fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "| USA|\n",
      "+----+\n",
      "|NULL|\n",
      "|NULL|\n",
      "|-196|\n",
      "|NULL|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted.where(\"date>'2011-12-05'\").select(\"USA\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31f7df6-c7dd-4fca-a50d-4df1659b7516",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
