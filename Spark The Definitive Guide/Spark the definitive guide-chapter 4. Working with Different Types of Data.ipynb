{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "522f811a-c387-438b-9ff7-b8c01681ffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62df9f19-d0c5-4b7f-8fa6-48fe49c15c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"My app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e2d804-2904-4cdf-a781-3f49bebbe14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .option(\"inferSchema\",\"true\")\\\n",
    "    .load(\"retail-data/by-day/2010-12-01.csv\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb3de29-4d65-42dc-8cf6-d9900c24b87d",
   "metadata": {},
   "source": [
    "### Boolean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e917045b-4858-4aa6-87ed-19bcaf5ae59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------------------------------+\n",
      "|InvoiceNo|Description                        |\n",
      "+---------+-----------------------------------+\n",
      "|536365   |WHITE HANGING HEART T-LIGHT HOLDER |\n",
      "|536365   |WHITE METAL LANTERN                |\n",
      "|536365   |CREAM CUPID HEARTS COAT HANGER     |\n",
      "|536365   |KNITTED UNION FLAG HOT WATER BOTTLE|\n",
      "|536365   |RED WOOLLY HOTTIE WHITE HEART.     |\n",
      "+---------+-----------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "df.where(col(\"InvoiceNo\")!=56365)\\\n",
    "    .select(\"InvoiceNo\",\"Description\")\\\n",
    "    .show(5,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f16491c-0938-47be-bf7b-f688ed098ce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|   Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|       1|2010-12-01 14:32:00|   569.77|      NULL|United Kingdom|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|       1|2010-12-01 17:06:00|   607.49|      NULL|United Kingdom|\n",
      "+---------+---------+--------------+--------+-------------------+---------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "priceFilter=col(\"UnitPrice\")>600\n",
    "descripFilter=instr(df.Description,\"POSTAGE\")>=1\n",
    "\n",
    "df.where(df.StockCode.isin(\"DOT\"))\\\n",
    "    .where(priceFilter | descripFilter)\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9139f629-4b4b-4b05-bee3-16a9a0cd04b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+--------------+\n",
      "|unitPrice|isExpensive|   Description|\n",
      "+---------+-----------+--------------+\n",
      "|   569.77|       true|DOTCOM POSTAGE|\n",
      "|   607.49|       true|DOTCOM POSTAGE|\n",
      "+---------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "DOTCodeFilter = col(\"StockCode\")==\"DOT\"\n",
    "priceFilter=col(\"UnitPrice\")>600\n",
    "descripFilter=instr(col(\"Description\"),\"POSTAGE\") >=1\n",
    "\n",
    "df.withColumn(\"isExpensive\",DOTCodeFilter &(priceFilter|descripFilter))\\\n",
    "    .where(\"isExpensive\")\\\n",
    "    .select(\"unitPrice\",\"isExpensive\",\"Description\")\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d41d82e-b370-4086-9924-3b519c21a814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Description|UnitPrice|\n",
      "+--------------------+---------+\n",
      "|RUSTIC  SEVENTEEN...|    165.0|\n",
      "|      DOTCOM POSTAGE|   569.77|\n",
      "|      DOTCOM POSTAGE|   607.49|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.withColumn(\"isExpensive\",expr(\"NOT UnitPrice <=100\"))\\\n",
    "    .where(\"isExpensive\")\\\n",
    "    .select(\"Description\",\"UnitPrice\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb4ee63-711f-4567-80d3-b863ad86dc5d",
   "metadata": {},
   "source": [
    "### Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c730389e-400c-4b46-ba8b-f1ef96c110c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|234.08999999999997|\n",
      "|   17850.0|          413.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr,pow\n",
    "\n",
    "fabricatedQuantity = pow(col(\"Quantity\")*col(\"UnitPrice\"),2)\n",
    "\n",
    "df.select(\n",
    "    expr(\"CustomerId\"),\n",
    "    fabricatedQuantity.alias(\"realQuantity\"))\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35129c9c-3eaf-4a49-a632-59858d72c9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|      realQuantity|\n",
      "+----------+------------------+\n",
      "|   17850.0|239.08999999999997|\n",
      "|   17850.0|          418.7156|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    \"CustomerId\",\n",
    "    \"(POWER((Quantity * UnitPrice),2.0)+5)as realQuantity\")\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a733dd35-4591-4ee5-8db1-72b493576886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit,round,bround\n",
    "\n",
    "df.select(\n",
    "    round(lit(\"2.5\")),\n",
    "    bround(lit(\"2.5\")))\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ffd2acd-ec90-4ff8-ab51-da74b0449702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|corr(Quantity, UnitPrice)|\n",
      "+-------------------------+\n",
      "|     -0.04112314436835551|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pearson Correlation coefficient\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "df.stat.corr(\"Quantity\",\"UnitPrice\")\n",
    "df.select(corr(\"Quantity\",\"UnitPrice\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43c04b7e-fa23-4d15-ada5-c293a38ebbd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|summary|        InvoiceNo|         StockCode|         Description|          Quantity|         UnitPrice|        CustomerID|       Country|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "|  count|             3108|              3108|                3098|              3108|              3108|              1968|          3108|\n",
      "|   mean| 536516.684944841|27834.304044117645|                NULL| 8.627413127413128| 4.151946589446603|15661.388719512195|          NULL|\n",
      "| stddev|72.89447869788873|17407.897548583845|                NULL|26.371821677029203|15.638659854603892|1854.4496996893627|          NULL|\n",
      "|    min|           536365|             10002| 4 PURPLE FLOCK D...|               -24|               0.0|           12431.0|     Australia|\n",
      "|    max|          C536548|              POST|ZINC WILLIE WINKI...|               600|            607.49|           18229.0|United Kingdom|\n",
      "+-------+-----------------+------------------+--------------------+------------------+------------------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52c34f9b-e7d8-44eb-bf16-b467843ce51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count,mean,stddev_pop,min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8dfc48b9-b865-4d9c-af0b-fa18fb455872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.51]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colName=\"UnitPrice\"\n",
    "quantileProbs=[0.5]\n",
    "relError=0.05\n",
    "df.stat.approxQuantile(\"UnitPrice\",quantileProbs,relError)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e515beb-06ce-477d-a448-e212dfede2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|StockCode_Quantity| -1|-10|-12| -2|-24| -3| -4| -5| -6| -7|  1| 10|100| 11| 12|120|128| 13| 14|144| 15| 16| 17| 18| 19|192|  2| 20|200| 21|216| 22| 23| 24| 25|252| 27| 28|288|  3| 30| 32| 33| 34| 36|384|  4| 40|432| 47| 48|480|  5| 50| 56|  6| 60|600| 64|  7| 70| 72|  8| 80|  9| 96|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "|             21259|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21894|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21452|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22728|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|\n",
      "|             21889|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21249|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22121|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|            90210B|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             90022|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21671|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22130|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22314|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21711|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22629|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  3|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22529|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             21967|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             82486|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  4|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             85064|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  2|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|             22438|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "|            85170D|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  1|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|  0|\n",
      "+------------------+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.crosstab(\"StockCode\",\"Quantity\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0bdaf94-6447-44dc-96ba-07b6184c5e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "| StockCode_freqItems|  Quantity_freqItems|\n",
      "+--------------------+--------------------+\n",
      "|[22086, 21705, 72...|[200, 128, 23, 50...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.freqItems([\"StockCode\",\"Quantity\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37004aeb-ec11-477d-890c-5490b4efc113",
   "metadata": {},
   "source": [
    "### Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8f1ccc9-077a-46b9-bd81-e2f78bdb027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|initcap(Description)|\n",
      "+--------------------+\n",
      "|White Hanging Hea...|\n",
      "| White Metal Lantern|\n",
      "|Cream Cupid Heart...|\n",
      "|Knitted Union Fla...|\n",
      "|Red Woolly Hottie...|\n",
      "|Set 7 Babushka Ne...|\n",
      "|Glass Star Froste...|\n",
      "|Hand Warmer Union...|\n",
      "|Hand Warmer Red P...|\n",
      "|Assorted Colour B...|\n",
      "|Poppy's Playhouse...|\n",
      "|Poppy's Playhouse...|\n",
      "|Feltcraft Princes...|\n",
      "|Ivory Knitted Mug...|\n",
      "|Box Of 6 Assorted...|\n",
      "|Box Of Vintage Ji...|\n",
      "|Box Of Vintage Al...|\n",
      "|Home Building Blo...|\n",
      "|Love Building Blo...|\n",
      "|Recipe Box With M...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import initcap\n",
    "\n",
    "df.select(initcap(col(\"Description\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aec8cdaf-3abe-4365-84ae-7c60a0a14f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|         Description|  lower(Description)|  upper(Description)|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|white hanging hea...|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| white metal lantern| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lower,upper\n",
    "\n",
    "df.select(\n",
    "    col(\"Description\"),\n",
    "    lower(col(\"Description\")),\n",
    "    upper(col(\"Description\")))\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97ce9995-1528-415f-985a-f100225827bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+-----+---+----------+\n",
      "|    ltrim|   rtrim| trim| lp|        rp|\n",
      "+---------+--------+-----+---+----------+\n",
      "|HELLO    |   HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO    |   HELLO|HELLO|HEL|HELLO     |\n",
      "+---------+--------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit,ltrim,rtrim,rpad,lpad,trim\n",
    "\n",
    "df.select(\n",
    "    ltrim(lit(\"   HELLO    \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"   HELLO    \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"   HELLO    \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"),3,\" \").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"),10,\" \").alias(\"rp\"))\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3811aa-4e31-40de-886f-0d07886e8700",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f93b2d8-0903-47c4-9183-729d7a6a4362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|       color_cleaned|         Description|\n",
      "+--------------------+--------------------+\n",
      "|COLOR HANGING HEA...|WHITE HANGING HEA...|\n",
      "| COLOR METAL LANTERN| WHITE METAL LANTERN|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#regular expression\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "\n",
    "regex_string =\"BLACK|WHITE|RED|GREEN|BLUE\"\n",
    "\n",
    "df.select(\n",
    "    regexp_replace(col(\"Description\"),regex_string,\"COLOR\")\n",
    "    .alias(\"color_cleaned\"),\n",
    "    col(\"Description\"))\\\n",
    "   .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "019c7c26-12e1-4d1d-af99-057c64ceaa7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------+\n",
      "|translate(Description, LEET, 1337)|         Description|\n",
      "+----------------------------------+--------------------+\n",
      "|              WHI73 HANGING H3A...|WHITE HANGING HEA...|\n",
      "|               WHI73 M37A1 1AN73RN| WHITE METAL LANTERN|\n",
      "+----------------------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import translate\n",
    "\n",
    "df.select(\n",
    "    translate(col(\"Description\"),\"LEET\",\"1337\"),\n",
    "    col(\"Description\"))\\\n",
    "   .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9b187d3c-e369-4a48-a5fb-386866d8b6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+\n",
      "|color_cleaned|         Description|\n",
      "+-------------+--------------------+\n",
      "|        WHITE|WHITE HANGING HEA...|\n",
      "|        WHITE| WHITE METAL LANTERN|\n",
      "+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "extract_str = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "df.select(\n",
    "    regexp_extract(col(\"Description\"),extract_str,1)\n",
    "    .alias(\"color_cleaned\"),\n",
    "    col(\"Description\"))\\\n",
    "   .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "872d7e93-c61e-4655-91c8-5f6266dc4470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import instr\n",
    "\n",
    "containsBlack = instr(col(\"Description\"),\"BLACK\")>=1\n",
    "containsWhite = instr(col(\"Description\"),\"WHITE\")>=1\n",
    "\n",
    "df.withColumn(\"hasSimpleColor\",containsBlack|containsWhite)\\\n",
    "    .filter(\"hasSimpleColor\")\\\n",
    "    .select(\"Description\")\\\n",
    "    .show(3,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ebbdc15-2a5a-4f0d-9964-33e7c39de120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|Description                       |\n",
      "+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.    |\n",
      "+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr, locate\n",
    "\n",
    "simpleColors = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "\n",
    "def color_locator(column, color_string):\n",
    "    \"\"\"This function creates a column declaring whether or\n",
    "    not a given pySpark column contains the UPPERCASED\n",
    "    color.\n",
    "    Returns a new column type that can be used\n",
    "    in a select statement.\n",
    "    \"\"\"\n",
    "    return locate(color_string.upper(), column)\\\n",
    "        .cast(\"boolean\")\\\n",
    "        .alias(\"is_\" + color_string)  # ← Correct: use color_string, not c\n",
    "\n",
    "# ✅ Fix 1: Close the list comprehension properly\n",
    "selectedColumns = [color_locator(df.Description, c) for c in simpleColors]\n",
    "selectedColumns.append(expr(\"*\"))  # Include all other columns too\n",
    "\n",
    "# ✅ Working Query\n",
    "df\\\n",
    "    .select(*selectedColumns)\\\n",
    "    .where(expr(\"is_white OR is_red\"))\\\n",
    "    .select(\"Description\")\\\n",
    "    .show(3, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6753ad68-5f17-43ff-9f3b-6355093f250e",
   "metadata": {},
   "source": [
    "### Dates and Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "134ba54b-b73a-4ff9-8634-adfec7a52b20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: timestamp (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd55e580-c831-4193-a965-df0d4f575437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Date and timestamp\n",
    "\n",
    "from pyspark.sql.functions import current_date,current_timestamp\n",
    "\n",
    "dateDF= spark.range(10)\\\n",
    "    .withColumn(\"today\",current_date())\\\n",
    "    .withColumn(\"now\",current_timestamp())\n",
    "\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")\n",
    "\n",
    "dateDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "75f38998-ba79-47ef-b4b9-6cff20e4f48e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2025-07-22|2025-07-22 14:49:...|\n",
      "|  1|2025-07-22|2025-07-22 14:49:...|\n",
      "|  2|2025-07-22|2025-07-22 14:49:...|\n",
      "|  3|2025-07-22|2025-07-22 14:49:...|\n",
      "|  4|2025-07-22|2025-07-22 14:49:...|\n",
      "|  5|2025-07-22|2025-07-22 14:49:...|\n",
      "|  6|2025-07-22|2025-07-22 14:49:...|\n",
      "|  7|2025-07-22|2025-07-22 14:49:...|\n",
      "|  8|2025-07-22|2025-07-22 14:49:...|\n",
      "|  9|2025-07-22|2025-07-22 14:49:...|\n",
      "+---+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "92a1ccbe-3605-4499-baf5-f9e15e9b0b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2025-07-17|        2025-07-27|\n",
      "|        2025-07-17|        2025-07-27|\n",
      "|        2025-07-17|        2025-07-27|\n",
      "|        2025-07-17|        2025-07-27|\n",
      "|        2025-07-17|        2025-07-27|\n",
      "|        2025-07-17|        2025-07-27|\n",
      "|        2025-07-17|        2025-07-27|\n",
      "|        2025-07-17|        2025-07-27|\n",
      "|        2025-07-17|        2025-07-27|\n",
      "|        2025-07-17|        2025-07-27|\n",
      "+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_add,date_sub\n",
    "\n",
    "dateDF\\\n",
    "    .select(\n",
    "        date_sub(col(\"today\"),5),\n",
    "        date_add(col(\"today\"),5))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "882c196d-f044-4c39-ac3c-e915d8a9ad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import datediff,months_between,to_date\n",
    "\n",
    "dateDF\\\n",
    "    .withColumn(\"week_ago\",date_sub(col(\"today\"),7))\\\n",
    "    .select(datediff(col(\"week_ago\"),col(\"today\")))\\\n",
    "    .show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8d6b9d9-57de-4a9f-9c2f-d7c70b44e872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                    -16.67741935|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF\\\n",
    "    .select(\n",
    "        to_date(lit(\"2016-01-01\")).alias(\"start\"),\n",
    "        to_date(lit(\"2017-05-22\")).alias(\"end\"))\\\n",
    "    .select(months_between(col(\"start\"),col(\"end\")))\\\n",
    "    .show(1)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3662b09c-1677-443c-b692-783d9f3c66cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|to_date(date)|\n",
      "+-------------+\n",
      "|   2017-01-01|\n",
      "|   2017-01-01|\n",
      "|   2017-01-01|\n",
      "|   2017-01-01|\n",
      "|   2017-01-01|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date,lit\n",
    "\n",
    "spark.range(5).withColumn(\"date\",lit(\"2017-01-01\"))\\\n",
    "    .select(to_date(col(\"date\")))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1b635f17-9cff-4e2e-8f31-d4df4684060a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|to_date(2016-20-12)|to_date(2017-12-11)|\n",
      "+-------------------+-------------------+\n",
      "|               NULL|         2017-12-11|\n",
      "|               NULL|         2017-12-11|\n",
      "|               NULL|         2017-12-11|\n",
      "|               NULL|         2017-12-11|\n",
      "|               NULL|         2017-12-11|\n",
      "|               NULL|         2017-12-11|\n",
      "|               NULL|         2017-12-11|\n",
      "|               NULL|         2017-12-11|\n",
      "|               NULL|         2017-12-11|\n",
      "|               NULL|         2017-12-11|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(to_date(lit(\"2016-20-12\")),to_date(lit(\"2017-12-11\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "81d0a70a-c3f7-4d59-a29a-4c3f92a59f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|      date|     date2|\n",
      "+----------+----------+\n",
      "|2017-11-12|2017-12-20|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for fixing above date bug we can use unix_timestamp\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "\n",
    "dateFormat=\"yyyy-dd-MM\"\n",
    "\n",
    "cleanDateDF= spark.range(1)\\\n",
    "    .select(\n",
    "        to_date(unix_timestamp(lit(\"2017-12-11\"),dateFormat).cast(\"timestamp\")\n",
    "               ).alias(\"date\"),\n",
    "        to_date(unix_timestamp(lit(\"2017-20-12\"),dateFormat).cast(\"timestamp\")\n",
    "               ).alias(\"date2\")\n",
    "    )\n",
    "\n",
    "cleanDateDF.show()\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cb4264e0-b783-4951-b11a-051b68ec372b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------+\n",
      "|CAST(unix_timestamp(date, yyyy-dd-MM) AS TIMESTAMP)|\n",
      "+---------------------------------------------------+\n",
      "|                                2017-11-12 00:00:00|\n",
      "+---------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cleanDateDF\\\n",
    "    .select(\n",
    "        unix_timestamp(col(\"date\"),dateFormat).cast(\"timestamp\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e80bc-fd90-47cf-95ba-62445c0c76bb",
   "metadata": {},
   "source": [
    "### Working with Nulls in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "81c6adbc-995c-4639-98ae-57cabe8fd9a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Working with Nulls in Data\n",
    "#Drop\n",
    "df.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "32a24226-1860-423b-9f2b-28dbe44d3295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2401e502-abde-4ac1-8117-eb21dd47548b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.drop(\"all\",subset=[\"StockCode\",\"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54241e21-5d0d-4811-a25e-a3aa92cda96f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fill\n",
    "df.na.fill(\"All Null Values become this string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a696601e-d4d5-4428-8eb3-bd2475d9c524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|   17850.0|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "|   536365|    22752|SET 7 BABUSHKA NE...|       2|2010-12-01 08:26:00|     7.65|   17850.0|United Kingdom|\n",
      "|   536365|    21730|GLASS STAR FROSTE...|       6|2010-12-01 08:26:00|     4.25|   17850.0|United Kingdom|\n",
      "|   536366|    22633|HAND WARMER UNION...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536366|    22632|HAND WARMER RED P...|       6|2010-12-01 08:28:00|     1.85|   17850.0|United Kingdom|\n",
      "|   536367|    84879|ASSORTED COLOUR B...|      32|2010-12-01 08:34:00|     1.69|   13047.0|United Kingdom|\n",
      "|   536367|    22745|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22748|POPPY'S PLAYHOUSE...|       6|2010-12-01 08:34:00|      2.1|   13047.0|United Kingdom|\n",
      "|   536367|    22749|FELTCRAFT PRINCES...|       8|2010-12-01 08:34:00|     3.75|   13047.0|United Kingdom|\n",
      "|   536367|    22310|IVORY KNITTED MUG...|       6|2010-12-01 08:34:00|     1.65|   13047.0|United Kingdom|\n",
      "|   536367|    84969|BOX OF 6 ASSORTED...|       6|2010-12-01 08:34:00|     4.25|   13047.0|United Kingdom|\n",
      "|   536367|    22623|BOX OF VINTAGE JI...|       3|2010-12-01 08:34:00|     4.95|   13047.0|United Kingdom|\n",
      "|   536367|    22622|BOX OF VINTAGE AL...|       2|2010-12-01 08:34:00|     9.95|   13047.0|United Kingdom|\n",
      "|   536367|    21754|HOME BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21755|LOVE BUILDING BLO...|       3|2010-12-01 08:34:00|     5.95|   13047.0|United Kingdom|\n",
      "|   536367|    21777|RECIPE BOX WITH M...|       4|2010-12-01 08:34:00|     7.95|   13047.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f330229f-d63a-4f82-91cf-5925bce639b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.na.fill(\"all\",subset=[\"StockCode\",\"InvoiceNo\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "30d7793d-33ea-4a21-919c-9f24327bace1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_cols_vals= {\n",
    "    \"StockCode\":5,\n",
    "    \"Description\":\"No Value\"\n",
    "}\n",
    "df.na.fill(fill_cols_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1051ae6c-aebb-4dd2-9b5c-7a7a5b797c27",
   "metadata": {},
   "source": [
    "## Replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da1bcab3-2bad-4738-8cfe-b0a541cd0fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[InvoiceNo: string, StockCode: string, Description: string, Quantity: int, InvoiceDate: timestamp, UnitPrice: double, CustomerID: double, Country: string]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace\n",
    "df.na.replace([\"\"],[\"UNKNOWN\"],\"Description\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e09b6-7dac-4537-8509-6ed2612d9172",
   "metadata": {},
   "source": [
    "## Complex types: structs, arrays and maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03aec959-9ff1-493d-bad4-c88d0ffcf85e",
   "metadata": {},
   "source": [
    "## Structs\n",
    "DataFrames within DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "92ef776b-12f4-493f-8d1b-525d29621b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+\n",
      "|complex                                      |\n",
      "+---------------------------------------------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER, 536365} |\n",
      "|{WHITE METAL LANTERN, 536365}                |\n",
      "|{CREAM CUPID HEARTS COAT HANGER, 536365}     |\n",
      "|{KNITTED UNION FLAG HOT WATER BOTTLE, 536365}|\n",
      "|{RED WOOLLY HOTTIE WHITE HEART., 536365}     |\n",
      "|{SET 7 BABUSHKA NESTING BOXES, 536365}       |\n",
      "|{GLASS STAR FROSTED T-LIGHT HOLDER, 536365}  |\n",
      "|{HAND WARMER UNION JACK, 536366}             |\n",
      "|{HAND WARMER RED POLKA DOT, 536366}          |\n",
      "|{ASSORTED COLOUR BIRD ORNAMENT, 536367}      |\n",
      "+---------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "complexDF=df\\\n",
    "    .select(struct(\"Description\",\"InvoiceNo\").alias(\"complex\"))\n",
    "\n",
    "complexDF.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e3d31af2-7759-4085-97dd-797a80d4bb83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "|CREAM CUPID HEART...|\n",
      "|KNITTED UNION FLA...|\n",
      "|RED WOOLLY HOTTIE...|\n",
      "|SET 7 BABUSHKA NE...|\n",
      "|GLASS STAR FROSTE...|\n",
      "|HAND WARMER UNION...|\n",
      "|HAND WARMER RED P...|\n",
      "|ASSORTED COLOUR B...|\n",
      "|POPPY'S PLAYHOUSE...|\n",
      "|POPPY'S PLAYHOUSE...|\n",
      "|FELTCRAFT PRINCES...|\n",
      "|IVORY KNITTED MUG...|\n",
      "|BOX OF 6 ASSORTED...|\n",
      "|BOX OF VINTAGE JI...|\n",
      "|BOX OF VINTAGE AL...|\n",
      "|HOME BUILDING BLO...|\n",
      "|LOVE BUILDING BLO...|\n",
      "|RECIPE BOX WITH M...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(\"complex.Description\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1dea2aa5-4db4-4a5c-9fcc-051241b7d91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "| WHITE METAL LANTERN|   536365|\n",
      "|CREAM CUPID HEART...|   536365|\n",
      "|KNITTED UNION FLA...|   536365|\n",
      "|RED WOOLLY HOTTIE...|   536365|\n",
      "|SET 7 BABUSHKA NE...|   536365|\n",
      "|GLASS STAR FROSTE...|   536365|\n",
      "|HAND WARMER UNION...|   536366|\n",
      "|HAND WARMER RED P...|   536366|\n",
      "|ASSORTED COLOUR B...|   536367|\n",
      "|POPPY'S PLAYHOUSE...|   536367|\n",
      "|POPPY'S PLAYHOUSE...|   536367|\n",
      "|FELTCRAFT PRINCES...|   536367|\n",
      "|IVORY KNITTED MUG...|   536367|\n",
      "|BOX OF 6 ASSORTED...|   536367|\n",
      "|BOX OF VINTAGE JI...|   536367|\n",
      "|BOX OF VINTAGE AL...|   536367|\n",
      "|HOME BUILDING BLO...|   536367|\n",
      "|LOVE BUILDING BLO...|   536367|\n",
      "|RECIPE BOX WITH M...|   536367|\n",
      "+--------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complexDF.select(\"complex.*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046289f9-dfad-4f05-8c2e-12a163f1dd7a",
   "metadata": {},
   "source": [
    "## Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "55775456-429d-4df9-8933-73a0ce63b697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|split(Description,  , -1)               |\n",
      "+----------------------------------------+\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|\n",
      "|[WHITE, METAL, LANTERN]                 |\n",
      "+----------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "df.select(split(col(\"Description\"),\" \")).show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "abfa843f-0c39-4b6e-88a9-42c48759d818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|array_col[0]|\n",
      "+------------+\n",
      "|       WHITE|\n",
      "|       WHITE|\n",
      "+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(split(col(\"Description\"),\" \").alias(\"array_col\"))\\\n",
    "    .selectExpr(\"array_col[0]\")\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c9044-a6d4-4392-a56b-d46f9b83fecc",
   "metadata": {},
   "source": [
    "### Array Contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "81726c5f-0191-47d5-a9ef-d40ac143ccba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------+\n",
      "|array_contains(split(Description,  , -1), WHITE)|\n",
      "+------------------------------------------------+\n",
      "|                                            true|\n",
      "|                                            true|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                            true|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "|                                           false|\n",
      "+------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "df.select(array_contains(split(col(\"Description\"),\" \"),\"WHITE\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0699a0-b019-4b80-836e-51a9e3dff664",
   "metadata": {},
   "source": [
    "### Explode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c2dc025-a09e-4003-8e4f-9454b8b731d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+--------+\n",
      "|Description                       |InvoiceNo|exploded|\n",
      "+----------------------------------+---------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HANGING |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HEART   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |T-LIGHT |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |HOLDER  |\n",
      "+----------------------------------+---------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split,explode\n",
    "\n",
    "df.withColumn(\"splitted\",split(col(\"Description\"),\" \"))\\\n",
    "    .withColumn(\"exploded\",explode(col(\"splitted\")))\\\n",
    "    .select(\"Description\",\"InvoiceNo\",\"exploded\")\\\n",
    "    .show(5,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4296a2-91a0-4e24-b288-f6db81ffc9e5",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd4e1b0c-6c03-4a1a-b613-42dfcb6f0386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+-----------------------------------+------+\n",
      "|complex_map                                    |Description                        |value |\n",
      "+-----------------------------------------------+-----------------------------------+------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER -> 536365} |WHITE HANGING HEART T-LIGHT HOLDER |536365|\n",
      "|{WHITE METAL LANTERN -> 536365}                |WHITE METAL LANTERN                |536365|\n",
      "|{CREAM CUPID HEARTS COAT HANGER -> 536365}     |CREAM CUPID HEARTS COAT HANGER     |536365|\n",
      "|{KNITTED UNION FLAG HOT WATER BOTTLE -> 536365}|KNITTED UNION FLAG HOT WATER BOTTLE|536365|\n",
      "|{RED WOOLLY HOTTIE WHITE HEART. -> 536365}     |RED WOOLLY HOTTIE WHITE HEART.     |536365|\n",
      "|{SET 7 BABUSHKA NESTING BOXES -> 536365}       |SET 7 BABUSHKA NESTING BOXES       |536365|\n",
      "|{GLASS STAR FROSTED T-LIGHT HOLDER -> 536365}  |GLASS STAR FROSTED T-LIGHT HOLDER  |536365|\n",
      "|{HAND WARMER UNION JACK -> 536366}             |HAND WARMER UNION JACK             |536366|\n",
      "|{HAND WARMER RED POLKA DOT -> 536366}          |HAND WARMER RED POLKA DOT          |536366|\n",
      "|{ASSORTED COLOUR BIRD ORNAMENT -> 536367}      |ASSORTED COLOUR BIRD ORNAMENT      |536367|\n",
      "|{POPPY'S PLAYHOUSE BEDROOM  -> 536367}         |POPPY'S PLAYHOUSE BEDROOM          |536367|\n",
      "|{POPPY'S PLAYHOUSE KITCHEN -> 536367}          |POPPY'S PLAYHOUSE KITCHEN          |536367|\n",
      "|{FELTCRAFT PRINCESS CHARLOTTE DOLL -> 536367}  |FELTCRAFT PRINCESS CHARLOTTE DOLL  |536367|\n",
      "|{IVORY KNITTED MUG COSY  -> 536367}            |IVORY KNITTED MUG COSY             |536367|\n",
      "|{BOX OF 6 ASSORTED COLOUR TEASPOONS -> 536367} |BOX OF 6 ASSORTED COLOUR TEASPOONS |536367|\n",
      "|{BOX OF VINTAGE JIGSAW BLOCKS  -> 536367}      |BOX OF VINTAGE JIGSAW BLOCKS       |536367|\n",
      "|{BOX OF VINTAGE ALPHABET BLOCKS -> 536367}     |BOX OF VINTAGE ALPHABET BLOCKS     |536367|\n",
      "|{HOME BUILDING BLOCK WORD -> 536367}           |HOME BUILDING BLOCK WORD           |536367|\n",
      "|{LOVE BUILDING BLOCK WORD -> 536367}           |LOVE BUILDING BLOCK WORD           |536367|\n",
      "|{RECIPE BOX WITH METAL HEART -> 536367}        |RECIPE BOX WITH METAL HEART        |536367|\n",
      "+-----------------------------------------------+-----------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, create_map\n",
    "\n",
    "df_with_map = df.select(\n",
    "    create_map(col(\"Description\"), col(\"InvoiceNo\")).alias(\"complex_map\"),\n",
    "    col(\"Description\")\n",
    ")\n",
    "\n",
    "df_with_map.withColumn(\"value\", col(\"complex_map\")[col(\"Description\")]).show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6268d379-1f8d-43c5-9e1d-d6b4b681aefe",
   "metadata": {},
   "source": [
    "## JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3625a303-4631-4eb1-ae00-a64336236580",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonDF=spark.range(1)\\\n",
    "    .selectExpr(\"\"\"\n",
    "    '{\"myJSONKey\" :{\"myJSONValue\" : [1,2,3]}}' as jsonString\n",
    "    \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e1830a42-579f-4966-9987-451488c1b8b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+\n",
      "|jsonString                              |\n",
      "+----------------------------------------+\n",
      "|{\"myJSONKey\" :{\"myJSONValue\" : [1,2,3]}}|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonDF.show(2,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e959b6f2-d3bb-46b8-804e-154895defe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------------+\n",
      "|myJSONValue|myJSONKey              |\n",
      "+-----------+-----------------------+\n",
      "|[1,2,3]    |{\"myJSONValue\":[1,2,3]}|\n",
      "+-----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, get_json_object, json_tuple\n",
    "\n",
    "jsonDF.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue\").alias(\"myJSONValue\"),\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\").alias(\"myJSONKey\")\n",
    ").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ad43d821-ad6d-49c1-8085-0625b07480e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------+\n",
      "|to_json(myStruct)                                                        |\n",
      "+-------------------------------------------------------------------------+\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"}|\n",
      "+-------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#structType to JSON\n",
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo,Description) as myStruct\")\\\n",
    "    .select(to_json(col(\"myStruct\"))).show(1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f928ec27-c504-4830-8735-d11b892258ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------+--------------------------------------------------------------------------+\n",
      "|parsed                                       |newJSON                                                                   |\n",
      "+---------------------------------------------+--------------------------------------------------------------------------+\n",
      "|{536365, WHITE HANGING HEART T-LIGHT HOLDER} |{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"} |\n",
      "|{536365, WHITE METAL LANTERN}                |{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}                |\n",
      "|{536365, CREAM CUPID HEARTS COAT HANGER}     |{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}     |\n",
      "|{536365, KNITTED UNION FLAG HOT WATER BOTTLE}|{\"InvoiceNo\":\"536365\",\"Description\":\"KNITTED UNION FLAG HOT WATER BOTTLE\"}|\n",
      "|{536365, RED WOOLLY HOTTIE WHITE HEART.}     |{\"InvoiceNo\":\"536365\",\"Description\":\"RED WOOLLY HOTTIE WHITE HEART.\"}     |\n",
      "|{536365, SET 7 BABUSHKA NESTING BOXES}       |{\"InvoiceNo\":\"536365\",\"Description\":\"SET 7 BABUSHKA NESTING BOXES\"}       |\n",
      "|{536365, GLASS STAR FROSTED T-LIGHT HOLDER}  |{\"InvoiceNo\":\"536365\",\"Description\":\"GLASS STAR FROSTED T-LIGHT HOLDER\"}  |\n",
      "|{536366, HAND WARMER UNION JACK}             |{\"InvoiceNo\":\"536366\",\"Description\":\"HAND WARMER UNION JACK\"}             |\n",
      "|{536366, HAND WARMER RED POLKA DOT}          |{\"InvoiceNo\":\"536366\",\"Description\":\"HAND WARMER RED POLKA DOT\"}          |\n",
      "|{536367, ASSORTED COLOUR BIRD ORNAMENT}      |{\"InvoiceNo\":\"536367\",\"Description\":\"ASSORTED COLOUR BIRD ORNAMENT\"}      |\n",
      "|{536367, POPPY'S PLAYHOUSE BEDROOM }         |{\"InvoiceNo\":\"536367\",\"Description\":\"POPPY'S PLAYHOUSE BEDROOM \"}         |\n",
      "|{536367, POPPY'S PLAYHOUSE KITCHEN}          |{\"InvoiceNo\":\"536367\",\"Description\":\"POPPY'S PLAYHOUSE KITCHEN\"}          |\n",
      "|{536367, FELTCRAFT PRINCESS CHARLOTTE DOLL}  |{\"InvoiceNo\":\"536367\",\"Description\":\"FELTCRAFT PRINCESS CHARLOTTE DOLL\"}  |\n",
      "|{536367, IVORY KNITTED MUG COSY }            |{\"InvoiceNo\":\"536367\",\"Description\":\"IVORY KNITTED MUG COSY \"}            |\n",
      "|{536367, BOX OF 6 ASSORTED COLOUR TEASPOONS} |{\"InvoiceNo\":\"536367\",\"Description\":\"BOX OF 6 ASSORTED COLOUR TEASPOONS\"} |\n",
      "|{536367, BOX OF VINTAGE JIGSAW BLOCKS }      |{\"InvoiceNo\":\"536367\",\"Description\":\"BOX OF VINTAGE JIGSAW BLOCKS \"}      |\n",
      "|{536367, BOX OF VINTAGE ALPHABET BLOCKS}     |{\"InvoiceNo\":\"536367\",\"Description\":\"BOX OF VINTAGE ALPHABET BLOCKS\"}     |\n",
      "|{536367, HOME BUILDING BLOCK WORD}           |{\"InvoiceNo\":\"536367\",\"Description\":\"HOME BUILDING BLOCK WORD\"}           |\n",
      "|{536367, LOVE BUILDING BLOCK WORD}           |{\"InvoiceNo\":\"536367\",\"Description\":\"LOVE BUILDING BLOCK WORD\"}           |\n",
      "|{536367, RECIPE BOX WITH METAL HEART}        |{\"InvoiceNo\":\"536367\",\"Description\":\"RECIPE BOX WITH METAL HEART\"}        |\n",
      "+---------------------------------------------+--------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "parseSchema = StructType((\n",
    "    StructField(\"InvoiceNo\",StringType(),True),\n",
    "    StructField(\"Description\",StringType(),True)))\n",
    "\n",
    "# Convert Struct to JSON and Parse Back\n",
    "df.selectExpr(\"struct(InvoiceNo, Description) as myStruct\") \\\n",
    "    .select(to_json(col(\"myStruct\")).alias(\"newJSON\")) \\\n",
    "    .select(from_json(col(\"newJSON\"), parseSchema).alias(\"parsed\"), col(\"newJSON\")) \\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cb1d5-36f7-43a9-8faf-36fd8ec30344",
   "metadata": {},
   "source": [
    "## User-Defined Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4e9c0615-9fc5-4895-9ac4-a361e27125de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udfExampleDF = spark.range(5).toDF(\"num\")\n",
    "\n",
    "def power3(double_value):\n",
    "    return double_value **3\n",
    "\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3151b656-e008-4f50-8d91-484769ff7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "power3udf = udf(power3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ae1414b9-4a4d-4f2f-b602-408a27d4e058",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.python.PythonFunction. Trace:\npy4j.Py4JException: Constructor org.apache.spark.api.python.PythonFunction([class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.String, class java.lang.String, class java.util.ArrayList, class org.apache.spark.api.python.PythonAccumulatorV2]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m udfExampleDF\u001b[38;5;241m.\u001b[39mselect(power3udf(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\sql\\udf.py:199\u001b[0m, in \u001b[0;36mUserDefinedFunction._wrapped.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, assigned\u001b[38;5;241m=\u001b[39massignments)\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m--> 199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\sql\\udf.py:177\u001b[0m, in \u001b[0;36mUserDefinedFunction.__call__\u001b[1;34m(self, *cols)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols):\n\u001b[1;32m--> 177\u001b[0m     judf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf\n\u001b[0;32m    178\u001b[0m     sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(judf\u001b[38;5;241m.\u001b[39mapply(_to_seq(sc, cols, _to_java_column)))\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\sql\\udf.py:161\u001b[0m, in \u001b[0;36mUserDefinedFunction._judf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_judf\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# It is possible that concurrent access, to newly created UDF,\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# will initialize multiple UserDefinedPythonFunctions.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;66;03m# This is unlikely, doesn't affect correctness,\u001b[39;00m\n\u001b[0;32m    159\u001b[0m     \u001b[38;5;66;03m# and should have a minimal performance impact.\u001b[39;00m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 161\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_judf()\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_judf_placeholder\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\sql\\udf.py:170\u001b[0m, in \u001b[0;36mUserDefinedFunction._create_judf\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    167\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[0;32m    168\u001b[0m sc \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\n\u001b[1;32m--> 170\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m _wrap_function(sc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnType)\n\u001b[0;32m    171\u001b[0m jdt \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mparseDataType(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturnType\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m    172\u001b[0m judf \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39morg\u001b[38;5;241m.\u001b[39mapache\u001b[38;5;241m.\u001b[39mspark\u001b[38;5;241m.\u001b[39msql\u001b[38;5;241m.\u001b[39mexecution\u001b[38;5;241m.\u001b[39mpython\u001b[38;5;241m.\u001b[39mUserDefinedPythonFunction(\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name, wrapped_func, jdt, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevalType, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeterministic)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\sql\\udf.py:35\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, returnType)\u001b[0m\n\u001b[0;32m     33\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, returnType)\n\u001b[0;32m     34\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\u001b[38;5;28mbytearray\u001b[39m(pickled_command), env, includes, sc\u001b[38;5;241m.\u001b[39mpythonExec,\n\u001b[0;32m     36\u001b[0m                               sc\u001b[38;5;241m.\u001b[39mpythonVer, broadcast_vars, sc\u001b[38;5;241m.\u001b[39m_javaAccumulator)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\py4j\\java_gateway.py:1568\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1562\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_command_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1564\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1565\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1567\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1568\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1569\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fqn)\n\u001b[0;32m   1571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1572\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\sql\\utils.py:111\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m py4j\u001b[38;5;241m.\u001b[39mprotocol\u001b[38;5;241m.\u001b[39mPy4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    113\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\py4j\\protocol.py:330\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n\u001b[0;32m    333\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    334\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    335\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    336\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n",
      "\u001b[1;31mPy4JError\u001b[0m: An error occurred while calling None.org.apache.spark.api.python.PythonFunction. Trace:\npy4j.Py4JException: Constructor org.apache.spark.api.python.PythonFunction([class [B, class java.util.HashMap, class java.util.ArrayList, class java.lang.String, class java.lang.String, class java.util.ArrayList, class org.apache.spark.api.python.PythonAccumulatorV2]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:750)\r\n\n"
     ]
    }
   ],
   "source": [
    "udfExampleDF.select(power3udf(col(\"num\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89021e6c-3e95-4ece-9319-c8d91bf972e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586605fd-2583-4985-976a-c431319f3e15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
