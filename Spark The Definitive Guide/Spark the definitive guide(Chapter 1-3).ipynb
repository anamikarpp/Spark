{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1836a556-89a7-4b65-93d7-4582e4d5beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab15dc4-4fab-4277-8356-797c4b1ea969",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"My app\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f03ff1b-bb68-4064-8077-0b2b7644cf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRange=spark.range(1000).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9e8e34e-0c6e-4e73-a9fa-37ba220d6cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "divisBy2 = myRange.where(\"number%2=0\") #transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2633981-f0b8-43fa-9292-40451f5b0a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "divisBy2.count() #action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3beaba32-d9f9-416b-8cb5-dd438c3672c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015= spark.read.json('book-data/json/2015-summary.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b869b9d-c391-4d7b-91af-78b10585fa2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#action\n",
    "flightData2015.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47618786-d333-43a5-b8bd-6697ede99bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "FileScan json [DEST_COUNTRY_NAME#18,ORIGIN_COUNTRY_NAME#19,count#20L] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/lenovo/book-data/json/2015-summary.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015.explain() #plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d4c882c-b164-4cc9-9cfc-d02ac4f5503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sortedFlightData2015 = flightData2015.sort(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56caa440-4583-4700-8349-765e0a8ee446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#20L ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(count#20L ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=58]\n",
      "      +- FileScan json [DEST_COUNTRY_NAME#18,ORIGIN_COUNTRY_NAME#19,count#20L] Batched: false, DataFilters: [], Format: JSON, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/lenovo/book-data/json/2015-summary.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sortedFlightData2015.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "225cc042-43df-481a-810d-7248df973f04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedFlightData2015.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2079c360-2e94-491d-966e-b66ccec09832",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"true\").csv(\"book-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be03b94d-3442-4e69-9cbf-a35e849bdfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: int]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18f837a6-dbb9-48c9-abc6-4bb997d8bed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvSchema = spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(\"book-data/csv/2015-summary.csv\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a9b264b-69e6-4a8a-98ae-18aaf1612db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonSchema=spark.read.format(\"json\").load(\"book-data/json/2015-summary.json\").schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abd1cc52-ee72-4a1d-9336-a4b51a6eedd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,IntegerType,true)))\n",
      "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))\n"
     ]
    }
   ],
   "source": [
    "print(csvSchema)\n",
    "print(jsonSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d7e1b57-4601-42b4-9895-6a81f4632681",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015 = spark.read.schema(jsonSchema).option(\"header\",\"true\").csv(\"book-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1dfca42-673e-46b0-aa9d-b7a7f0f3429e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))\n"
     ]
    }
   ],
   "source": [
    "print(flightData2015.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00c7b7b4-a433-430d-9f57-e1d1588cd980",
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData2015.createOrReplaceTempView(\"flight_data_2015\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3307b58-45ee-44e6-86c1-fcf690bf9e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\" SELECT \n",
    "                            DEST_COUNTRY_NAME, \n",
    "                            COUNT(1) \n",
    "                        FROM\n",
    "                            flight_data_2015\n",
    "                        GROUP BY\n",
    "                            DEST_COUNTRY_NAME\n",
    "                    \"\"\")\n",
    "\n",
    "dataFrameWay = flightData2015.groupBy(\"DEST_COUNTRY_NAME\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f87adb50-ee8e-4cf3-b8f6-8790aeac3bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#91, 200), ENSURE_REQUIREMENTS, [plan_id=121]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#91] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/lenovo/book-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c041ede-ec8c-412c-8b45-8fee63f6608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[count(1)])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#91, 200), ENSURE_REQUIREMENTS, [plan_id=134]\n",
      "      +- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[partial_count(1)])\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#91] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/lenovo/book-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrameWay.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8141bf91-b223-4beb-a8bc-55f17f0e9410",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "flightData2015.select(max(\"count\")).take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccc5f695-49ec-40e9-b7a7-a8dfe4ea9884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT max(count) from flight_Data_2015\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96b17d7-512d-47b6-a11b-18483ce3fb46",
   "metadata": {},
   "source": [
    "### What are the top five destination countries in the data set? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5fec3050-13f1-4805-8ab0-eb96ad6a663c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', destination_total=411352),\n",
       " Row(DEST_COUNTRY_NAME='Canada', destination_total=8399),\n",
       " Row(DEST_COUNTRY_NAME='Mexico', destination_total=7140),\n",
       " Row(DEST_COUNTRY_NAME='United Kingdom', destination_total=2025),\n",
       " Row(DEST_COUNTRY_NAME='Japan', destination_total=1548)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxSql = spark.sql(\"\"\" \n",
    "                        SELECT \n",
    "                            DEST_COUNTRY_NAME,\n",
    "                            SUM(count) as destination_total\n",
    "                        FROM\n",
    "                            flight_data_2015\n",
    "                        GROUP BY\n",
    "                            DEST_COUNTRY_NAME\n",
    "                        ORDER BY\n",
    "                            sum(count) DESC\n",
    "                        LIMIT \n",
    "                            5\n",
    "                    \"\"\")\n",
    "maxSql.collect()          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eb3c3b8f-c731-407a-8a37-8730f102edb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', destination_total=411352),\n",
       " Row(DEST_COUNTRY_NAME='Canada', destination_total=8399),\n",
       " Row(DEST_COUNTRY_NAME='Mexico', destination_total=7140),\n",
       " Row(DEST_COUNTRY_NAME='United Kingdom', destination_total=2025),\n",
       " Row(DEST_COUNTRY_NAME='Japan', destination_total=1548)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7b86535f-7c65-4f34-b127-5e330df5e7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- TakeOrderedAndProject(limit=5, orderBy=[destination_total#163L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#91,destination_total#163L])\n",
      "   +- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[sum(count#93L)])\n",
      "      +- Exchange hashpartitioning(DEST_COUNTRY_NAME#91, 200), ENSURE_REQUIREMENTS, [plan_id=304]\n",
      "         +- HashAggregate(keys=[DEST_COUNTRY_NAME#91], functions=[partial_sum(count#93L)])\n",
      "            +- FileScan csv [DEST_COUNTRY_NAME#91,count#93L] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/C:/Users/lenovo/book-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightData2015\\\n",
    "    .groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    "    .sum(\"count\")\\\n",
    "    .withColumnRenamed(\"sum(count)\",\"destination_total\")\\\n",
    "    .sort(desc(\"destination_total\"))\\\n",
    "    .limit(5)\\\n",
    "    .explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69989d4c-d349-4636-be12-df1629a7e14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "added10=df.select(df[\"number\"]+10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8bc2b70-32a6-4802-bec3-6062d226528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.range(500).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f7865492-1681-4a58-b79b-61f935984d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|(number + 10)|\n",
      "+-------------+\n",
      "|           10|\n",
      "|           11|\n",
      "|           12|\n",
      "|           13|\n",
      "|           14|\n",
      "|           15|\n",
      "|           16|\n",
      "|           17|\n",
      "|           18|\n",
      "|           19|\n",
      "|           20|\n",
      "|           21|\n",
      "|           22|\n",
      "|           23|\n",
      "|           24|\n",
      "|           25|\n",
      "|           26|\n",
      "|           27|\n",
      "|           28|\n",
      "|           29|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "added10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e5dacfb-41e7-4a0a-9229-11c7ad252353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(id,LongType,false)))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(500).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dae581be-7cfd-4475-b89e-d21e2c27e58a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bf65a01c-845d-4846-8011-58562d65eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "b=ByteType()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "874035ca-3bb5-47d2-8f2c-d44e3dd374f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").load(\"book-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8b8b338b-58d4-4092-a82e-3da9bff5cad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16333470-b4d9-4b95-88d0-ffc2e332df47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496dc6e-e1c3-49d7-b04c-c2879adf7804",
   "metadata": {},
   "source": [
    "### schema\n",
    "A schema is made up of structType which consists of lists of structfields(name,type,boolean null or not null)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86e40b1d-3a4e-402b-841a-fc986b64ed2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(number,LongType,false)))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84d822bc-5390-4c0b-b0b0-f9d892336cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manual schema\n",
    "from pyspark.sql.types import StructField,StructType,StringType,LongType\n",
    "\n",
    "myManualSchema= StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\",StringType(),True),\n",
    "    StructField(\"count\",LongType(),False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4b18f0c-b25e-4aba-bff9-a1fdb18b1366",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"json\")\\\n",
    "    .schema(myManualSchema)\\\n",
    "    .load(\"book-data/json/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56bc8e04-cf36-45e0-8fdb-db78e4b14360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13960cd-7a7c-4243-9875-636cefac9d41",
   "metadata": {},
   "source": [
    "### Columns and Expressions\n",
    "Columns as expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9125c9ee-08a0-49b6-a0c0-27b8aa85b6a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'someColumnNames'>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,column\n",
    "col(\"someColumnNames\")\n",
    "#column(\"someColumnNames\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d205ddd-d196-4820-a93c-ff7f7a777859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'((((somecol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "expr(\"(((somecol+5)*200)-6)<otherCol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7cf14c5-15db-49b3-adb9-9d47d45fb36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Accessing a DataFrame's Columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5200055-070f-406b-bf34-32e000a5438c",
   "metadata": {},
   "source": [
    "### Records and Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d1b09988-3470-40b4-8193-e749d4ef62e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First row\n",
    "df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0008648f-5c4d-4f6b-b78b-00d8117203d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating row\n",
    "from pyspark.sql import Row\n",
    "myRow = Row(\"Hello\",None,1,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "20e2bb87-ad95-4cc0-a8f2-a482d420fba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a664a8b-8ca0-4eeb-8913-e81eae4c88ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1376c12e-c475-4b08-9770-bf5af8b365d1",
   "metadata": {},
   "source": [
    "### Creating DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "af46459b-050e-4e89-8e82-caa1e564342e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.format(\"json\")\\\n",
    "    .load(\"book-data/json/2015-summary.json\")\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0095c4-7c7b-4a8e-91e1-40750ab5904f",
   "metadata": {},
   "source": [
    "#### select and selectexpr\n",
    "select : columns or expressions\n",
    "selectexpr : exprsions in strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c1c533eb-ff4d-4ecd-909b-e04989e9f10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "625c1705-1f35-403d-8982-40d1ec71734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States'),\n",
       " Row(DEST_COUNTRY_NAME='United States')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = spark.sql(\"SELECT DEST_COUNTRY_NAME FROM dfTable LIMIT 2\")\n",
    "result.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "665e34fc-91f4-49fb-8652-5a1d51c0d871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "|    United States|            Ireland|\n",
      "+-----------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"DEST_COUNTRY_NAME\",\"ORIGIN_COUNTRY_NAME\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7896e5e1-d3ca-4b4b-9de3-8660c981e19e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia'),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland')]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1=spark.sql(\"SELECT DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME FROM dfTable LIMIT 3\")\n",
    "result1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d4bc1ef4-3f4e-4130-bc01-338fdcfb4848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+\n",
      "|    United States|    United States|\n",
      "|    United States|    United States|\n",
      "+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr,col,column\n",
    "\n",
    "df.select(\n",
    "    expr(\"DEST_COUNTRY_NAME\"),\n",
    "    col(\"DEST_COUNTRY_NAME\"))\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a8490e5c-8082-4946-90ea-e8910fd1d359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|   DEST_COUNTRY_NAME|   DEST_COUNTRY_NAME|\n",
      "+--------------------+--------------------+\n",
      "|       United States|       United States|\n",
      "|       United States|       United States|\n",
      "|       United States|       United States|\n",
      "|               Egypt|               Egypt|\n",
      "|       United States|       United States|\n",
      "|       United States|       United States|\n",
      "|       United States|       United States|\n",
      "|          Costa Rica|          Costa Rica|\n",
      "|             Senegal|             Senegal|\n",
      "|             Moldova|             Moldova|\n",
      "|       United States|       United States|\n",
      "|       United States|       United States|\n",
      "|              Guyana|              Guyana|\n",
      "|               Malta|               Malta|\n",
      "|            Anguilla|            Anguilla|\n",
      "|             Bolivia|             Bolivia|\n",
      "|       United States|       United States|\n",
      "|             Algeria|             Algeria|\n",
      "|Turks and Caicos ...|Turks and Caicos ...|\n",
      "|       United States|       United States|\n",
      "+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " df.select(col(\"DEST_COUNTRY_NAME\"), \"DEST_COUNTRY_NAME\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b60a8ec0-9bbc-4a7d-8cca-da5ca492abb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|  destination|\n",
      "+-------------+\n",
      "|United States|\n",
      "|United States|\n",
      "+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2478fa0a-cff6-4119-8177-4b699d53d09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    expr(\"DEST_COUNTRY_NAME as destination\").alias(\"DEST_COUNTRY_NAME\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee457c02-a1e6-4a83-aeaf-fd96e3c17085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------------+\n",
      "|newColumnName|DEST_COUNTRY_NAME|\n",
      "+-------------+-----------------+\n",
      "|United States|    United States|\n",
      "|United States|    United States|\n",
      "+-------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    \"DEST_COUNTRY_NAME as newColumnName\",\n",
    "    \"DEST_COUNTRY_NAME\"\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4c24ae65-8f21-41e1-9ac2-585acb5f1f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\n",
    "    \"*\",\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\")\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e149331-1baa-47ba-89e9-537fb4a1e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------------------------+\n",
      "| avg(count)|count(DISTINCT DEST_COUNTRY_NAME)|\n",
      "+-----------+---------------------------------+\n",
      "|1770.765625|                              132|\n",
      "+-----------+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.selectExpr(\"avg(count)\",\"count(distinct(DEST_COUNTRY_NAME))\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea20ff9-eb6a-4864-aca5-f5ab5498d1e0",
   "metadata": {},
   "source": [
    "### Converting to Spark Types(Literals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "49f7fcb5-7554-4a28-92ae-f924929d9108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df.select(\n",
    "    expr(\"*\"),\n",
    "    lit(1).alias(\"One\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8352d-4d81-498d-94f8-6adbdaddb5a4",
   "metadata": {},
   "source": [
    "### Adding Columns\n",
    "withColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a80ff9e4-e56e-4763-b837-00e0e115684d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|    United States|            Romania|   15|        1|\n",
      "|    United States|            Croatia|    1|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"numberOne\",lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "79e22069-aead-4702-b039-1f843b0b34d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|    United States|            Romania|   15|        false|\n",
      "|    United States|            Croatia|    1|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\n",
    "    \"withinCountry\",\n",
    "    expr(\"ORIGIN_COUNTRY_NAME == DEST_COUNTRY_NAME\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426564d-1991-4bca-84a2-89890127758c",
   "metadata": {},
   "source": [
    "### Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7c2e6c83-c0d2-4e65-8f38-8a3b33debd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dest', 'ORIGIN_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.withColumnRenamed(\"DEST_COUNTRY_NAME\",\"dest\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48c2ef2-c79a-4ec8-9629-ad0b6b703c6b",
   "metadata": {},
   "source": [
    "### Reserved Characters and Keywords in Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ddaafe9e-daf6-4f45-adef-1c9044a1f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithLongColName =df\\\n",
    "    .withColumn(\n",
    "        \"This Long Column-Name\",\n",
    "        expr(\"ORIGIN_COUNTRY_NAME\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aebc307b-30ab-449d-8d52-d62b1a3f592d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-------+\n",
      "|This Long Column-Name|new col|\n",
      "+---------------------+-------+\n",
      "|              Romania|Romania|\n",
      "|              Croatia|Croatia|\n",
      "+---------------------+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfWithLongColName\\\n",
    ".selectExpr(\n",
    "    \"`This Long Column-Name`\",\n",
    "    \"`This Long Column-Name` as `new col`\")\\\n",
    "    .show(2)\n",
    "\n",
    "dfWithLongColName.createOrReplaceTempView(\"dfTableLong\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2672adf-148f-4eb2-89da-03ff467d3627",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This Long Column-Name']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.select(expr(\"`This Long Column-Name`\")).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1273a3bc-60b9-4675-a029-2af4b86e57f3",
   "metadata": {},
   "source": [
    "### Removing columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5829f9b5-1a0d-4912-857f-2cdbb381009e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DEST_COUNTRY_NAME', 'count']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing columns\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "82370abd-300c-42c4-801a-a7292bd6095d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count', 'This Long Column-Name']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#removing multiple columns\n",
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dfb9ca-754d-468a-9ec2-061d194d51c1",
   "metadata": {},
   "source": [
    "### Changing column's type(cast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "39fc12ef-c629-41c1-a115-2415d951fdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "022314c0-194b-470e-952e-3bc69170fb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#changing a column's type\n",
    "df.withColumn(\"count\",col(\"count\").cast(\"int\")).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dfdf25-72b8-46cb-bc94-ad9343889dd4",
   "metadata": {},
   "source": [
    "### Filtering Rows\n",
    "To filter rows we create an expression that evaluates to true or false\n",
    "where,filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "db5c0cd8-d808-4e04-bcbf-c5d46da5af93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#filtering rows\n",
    "colCondition =df.filter(col(\"count\")<2).take(2)\n",
    "conditional =df.where(\"count<2\").take(2)\n",
    "colCondition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01d29930-1a07-4b49-9e31-e124fa688e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Malta', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Gibraltar', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Saint Vincent and the Grenadines', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Suriname', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Cyprus', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Burkina Faso', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Djibouti', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Estonia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Zambia', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Cyprus', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Lithuania', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bulgaria', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Georgia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Bahrain', count=1),\n",
       " Row(DEST_COUNTRY_NAME=\"Cote d'Ivoire\", ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Papua New Guinea', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Kosovo', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Iraq', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Indonesia', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='New Caledonia', ORIGIN_COUNTRY_NAME='United States', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Montenegro', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Namibia', count=1)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = spark.sql(\"SELECT * FROM dfTable WHERE count<2\")\n",
    "query.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "15e031d4-453e-4337-841f-5c2dc4d9707b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb49e556-8e9a-4d1d-98f3-077b5a79e6a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"count\")<2)\\\n",
    "    .where(col(\"ORIGIN_COUNTRY_NAME\") !=\"Croatia\")\\\n",
    "    .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f3136d3a-1e54-42d9-8e07-7e2f27f82782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|          Singapore|    1|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query=spark.sql(\"SELECT * FROM dfTable WHERE count<2 AND ORIGIN_COUNTRY_NAME!='Croatia'\")\n",
    "query.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6eb99f-d40c-4f40-bbb7-5bb7d5113e8d",
   "metadata": {},
   "source": [
    "### Getting unique Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "21b69e3d-23ce-42a9-a1fa-26900b4e57ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Getting unique Rows\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "47fe4e94-7d19-4841-a9b8-b44b5e0911dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(\"ORIGIN_COUNTRY_NAME\",\"DEST_COUNTRY_NAME\").distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bff358-702a-4990-9930-c22f999fbd69",
   "metadata": {},
   "source": [
    "### Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed2bc9ac-1dcd-4b8e-b0fc-9ef860a8916f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#random samples\n",
    "seed=5\n",
    "withReplacement = False\n",
    "fraction=0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b7dfc996-eff9-4084-882a-d808fad3344a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(withReplacement,fraction,seed).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4557e679-e551-4cee-8fdd-f3a33a3f0edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames = df.randomSplit([0.25,0.75],seed)\n",
    "dataFrames[0].count()>dataFrames[1].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bd80c6-0919-4760-8bc5-87847fca303b",
   "metadata": {},
   "source": [
    "### Concatenating and Appending Rows to a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4c2b96f9-9fc4-46bc-afaf-d1370162e157",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "schema=df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "c4a74ce2-2156-476f-94be-a6bb8249b581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"H:\\anaconda\\Lib\\site-packages\\pyspark\\serializers.py\", line 437, in dumps\n",
      "    return cloudpickle.dumps(obj, pickle_protocol)\n",
      "           ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"H:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 72, in dumps\n",
      "    cp.dump(obj)\n",
      "    ~~~~~~~^^^^^\n",
      "  File \"H:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 540, in dump\n",
      "    return Pickler.dump(self, obj)\n",
      "           ~~~~~~~~~~~~^^^^^^^^^^^\n",
      "  File \"H:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 630, in reducer_override\n",
      "    return self._function_reduce(obj)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "  File \"H:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 503, in _function_reduce\n",
      "    return self._dynamic_function_reduce(obj)\n",
      "           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^\n",
      "  File \"H:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 484, in _dynamic_function_reduce\n",
      "    state = _function_getstate(func)\n",
      "  File \"H:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py\", line 156, in _function_getstate\n",
      "    f_globals_ref = _extract_code_globals(func.__code__)\n",
      "  File \"H:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py\", line 236, in _extract_code_globals\n",
      "    out_names = {names[oparg] for _, oparg in _walk_global_ops(co)}\n",
      "                 ~~~~~^^^^^^^\n",
      "IndexError: tuple index out of range\n"
     ]
    },
    {
     "ename": "PicklingError",
     "evalue": "Could not serialize object: IndexError: tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\serializers.py:437\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cloudpickle\u001b[38;5;241m.\u001b[39mdumps(obj, pickle_protocol)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPickleError:\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:72\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[0;32m     69\u001b[0m cp \u001b[38;5;241m=\u001b[39m CloudPickler(\n\u001b[0;32m     70\u001b[0m     file, protocol\u001b[38;5;241m=\u001b[39mprotocol, buffer_callback\u001b[38;5;241m=\u001b[39mbuffer_callback\n\u001b[0;32m     71\u001b[0m )\n\u001b[1;32m---> 72\u001b[0m cp\u001b[38;5;241m.\u001b[39mdump(obj)\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m file\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:540\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 540\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Pickler\u001b[38;5;241m.\u001b[39mdump(\u001b[38;5;28mself\u001b[39m, obj)\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:630\u001b[0m, in \u001b[0;36mCloudPickler.reducer_override\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, types\u001b[38;5;241m.\u001b[39mFunctionType):\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_reduce(obj)\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[38;5;66;03m# fallback to save_global, including the Pickler's\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[38;5;66;03m# dispatch_table\u001b[39;00m\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:503\u001b[0m, in \u001b[0;36mCloudPickler._function_reduce\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 503\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dynamic_function_reduce(obj)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:484\u001b[0m, in \u001b[0;36mCloudPickler._dynamic_function_reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    483\u001b[0m newargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_getnewargs(func)\n\u001b[1;32m--> 484\u001b[0m state \u001b[38;5;241m=\u001b[39m _function_getstate(func)\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (types\u001b[38;5;241m.\u001b[39mFunctionType, newargs, state, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    486\u001b[0m         _function_setstate)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle_fast.py:156\u001b[0m, in \u001b[0;36m_function_getstate\u001b[1;34m(func)\u001b[0m\n\u001b[0;32m    145\u001b[0m slotstate \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__qualname__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__closure__\u001b[39m\u001b[38;5;124m\"\u001b[39m: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__closure__\u001b[39m,\n\u001b[0;32m    154\u001b[0m }\n\u001b[1;32m--> 156\u001b[0m f_globals_ref \u001b[38;5;241m=\u001b[39m _extract_code_globals(func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__code__\u001b[39m)\n\u001b[0;32m    157\u001b[0m f_globals \u001b[38;5;241m=\u001b[39m {k: func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m f_globals_ref \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m\n\u001b[0;32m    158\u001b[0m              func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__globals__\u001b[39m}\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\cloudpickle\\cloudpickle.py:236\u001b[0m, in \u001b[0;36m_extract_code_globals\u001b[1;34m(co)\u001b[0m\n\u001b[0;32m    235\u001b[0m names \u001b[38;5;241m=\u001b[39m co\u001b[38;5;241m.\u001b[39mco_names\n\u001b[1;32m--> 236\u001b[0m out_names \u001b[38;5;241m=\u001b[39m {names[oparg] \u001b[38;5;28;01mfor\u001b[39;00m _, oparg \u001b[38;5;129;01min\u001b[39;00m _walk_global_ops(co)}\n\u001b[0;32m    238\u001b[0m \u001b[38;5;66;03m# Declaring a function inside another one using the \"def ...\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# syntax generates a constant code object corresponding to the one\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;66;03m# of the nested function's As the nested function may itself need\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;66;03m# global variables, we need to introspect its code, extract its\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# globals, (look for code object in it's co_consts attribute..) and\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# add the result to code_globals\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: tuple index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 8\u001b[0m\n\u001b[0;32m      3\u001b[0m newRows \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      4\u001b[0m Row(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew Country\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOther Country\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m),\n\u001b[0;32m      5\u001b[0m Row(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNew Country 2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOther Country 3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m ]\n\u001b[0;32m      7\u001b[0m parallelizedRows \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39mparallelize(newRows)\n\u001b[1;32m----> 8\u001b[0m newDF \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame(parallelizedRows, schema)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\sql\\session.py:675\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pandas\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m    673\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(\n\u001b[0;32m    674\u001b[0m         data, schema, samplingRatio, verifySchema)\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_dataframe(data, schema, samplingRatio, verifySchema)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\sql\\session.py:701\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    700\u001b[0m     rdd, schema \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromLocal(\u001b[38;5;28mmap\u001b[39m(prepare, data), schema)\n\u001b[1;32m--> 701\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n\u001b[0;32m    702\u001b[0m jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jsparkSession\u001b[38;5;241m.\u001b[39mapplySchemaToPythonRDD(jrdd\u001b[38;5;241m.\u001b[39mrdd(), schema\u001b[38;5;241m.\u001b[39mjson())\n\u001b[0;32m    703\u001b[0m df \u001b[38;5;241m=\u001b[39m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrapped)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\rdd.py:2618\u001b[0m, in \u001b[0;36mRDD._to_java_object_rdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2612\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Return a JavaRDD of Object by unpickling\u001b[39;00m\n\u001b[0;32m   2613\u001b[0m \n\u001b[0;32m   2614\u001b[0m \u001b[38;5;124;03mIt will convert each Python object into Java object by Pyrolite, whenever the\u001b[39;00m\n\u001b[0;32m   2615\u001b[0m \u001b[38;5;124;03mRDD is serialized in batch or not.\u001b[39;00m\n\u001b[0;32m   2616\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2617\u001b[0m rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pickled()\n\u001b[1;32m-> 2618\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mpythonToJava(rdd\u001b[38;5;241m.\u001b[39m_jrdd, \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\rdd.py:2949\u001b[0m, in \u001b[0;36mPipelinedRDD._jrdd\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2946\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2947\u001b[0m     profiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2949\u001b[0m wrapped_func \u001b[38;5;241m=\u001b[39m _wrap_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd_deserializer,\n\u001b[0;32m   2950\u001b[0m                               \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer, profiler)\n\u001b[0;32m   2951\u001b[0m python_rdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_jrdd\u001b[38;5;241m.\u001b[39mrdd(), wrapped_func,\n\u001b[0;32m   2952\u001b[0m                                      \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreservesPartitioning, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_barrier)\n\u001b[0;32m   2953\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_val \u001b[38;5;241m=\u001b[39m python_rdd\u001b[38;5;241m.\u001b[39masJavaRDD()\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\rdd.py:2828\u001b[0m, in \u001b[0;36m_wrap_function\u001b[1;34m(sc, func, deserializer, serializer, profiler)\u001b[0m\n\u001b[0;32m   2826\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m serializer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserializer should not be empty\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2827\u001b[0m command \u001b[38;5;241m=\u001b[39m (func, profiler, deserializer, serializer)\n\u001b[1;32m-> 2828\u001b[0m pickled_command, broadcast_vars, env, includes \u001b[38;5;241m=\u001b[39m _prepare_for_python_RDD(sc, command)\n\u001b[0;32m   2829\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonFunction(\u001b[38;5;28mbytearray\u001b[39m(pickled_command), env, includes, sc\u001b[38;5;241m.\u001b[39mpythonExec,\n\u001b[0;32m   2830\u001b[0m                               sc\u001b[38;5;241m.\u001b[39mpythonVer, broadcast_vars, sc\u001b[38;5;241m.\u001b[39m_javaAccumulator)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\rdd.py:2814\u001b[0m, in \u001b[0;36m_prepare_for_python_RDD\u001b[1;34m(sc, command)\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_prepare_for_python_RDD\u001b[39m(sc, command):\n\u001b[0;32m   2812\u001b[0m     \u001b[38;5;66;03m# the serialized command will be compressed by broadcast\u001b[39;00m\n\u001b[0;32m   2813\u001b[0m     ser \u001b[38;5;241m=\u001b[39m CloudPickleSerializer()\n\u001b[1;32m-> 2814\u001b[0m     pickled_command \u001b[38;5;241m=\u001b[39m ser\u001b[38;5;241m.\u001b[39mdumps(command)\n\u001b[0;32m   2815\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pickled_command) \u001b[38;5;241m>\u001b[39m sc\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mgetBroadcastThreshold(sc\u001b[38;5;241m.\u001b[39m_jsc):  \u001b[38;5;66;03m# Default 1M\u001b[39;00m\n\u001b[0;32m   2816\u001b[0m         \u001b[38;5;66;03m# The broadcast will have same life cycle as created PythonRDD\u001b[39;00m\n\u001b[0;32m   2817\u001b[0m         broadcast \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mbroadcast(pickled_command)\n",
      "File \u001b[1;32mH:\\anaconda\\Lib\\site-packages\\pyspark\\serializers.py:447\u001b[0m, in \u001b[0;36mCloudPickleSerializer.dumps\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    445\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not serialize object: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (e\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, emsg)\n\u001b[0;32m    446\u001b[0m print_exec(sys\u001b[38;5;241m.\u001b[39mstderr)\n\u001b[1;32m--> 447\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mPicklingError(msg)\n",
      "\u001b[1;31mPicklingError\u001b[0m: Could not serialize object: IndexError: tuple index out of range"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "Row(\"New Country\", \"Other Country\", 5),\n",
    "Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a31225-9e8e-4878-ada5-3c235dc3af12",
   "metadata": {},
   "source": [
    "### Sorting Rows\n",
    "sort,orderBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "958db465-8ae9-4e27-8131-b222488dc89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Sorting rows\n",
    "df.sort(\"count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fd929cfe-ed5a-4825-8ad5-edc6eca48538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"count\",\"DEST_COUNTRY_NAME\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "a66a4b6c-942b-494f-a4d2-d3e6dcc97518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "|        Indonesia|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"count\"),col(\"DEST_COUNTRY_NAME\")).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "22137d43-4315-4c85-884c-d99cb51a8bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc,asc\n",
    "\n",
    "df.orderBy(desc(\"count\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d3d69538-a8b0-4b6b-9575-b68adbf58f15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| count|\n",
      "+------+\n",
      "|370002|\n",
      "|  8483|\n",
      "|  8399|\n",
      "|  7187|\n",
      "|  7140|\n",
      "|  2025|\n",
      "|  1970|\n",
      "|  1548|\n",
      "|  1496|\n",
      "|  1468|\n",
      "|  1420|\n",
      "|  1353|\n",
      "|  1336|\n",
      "|  1048|\n",
      "|   986|\n",
      "|   955|\n",
      "|   952|\n",
      "|   935|\n",
      "|   920|\n",
      "|   873|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"count\").distinct().orderBy(\"count\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "658f87d5-3f91-49e6-a79e-c5a7745956e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|    United States|             Canada|  8483|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(desc(col(\"count\")),asc(col(\"DEST_COUNTRY_NAME\"))).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "f351f7b9-11f0-4f0e-9a11-3f1e637204fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorting withing partition for optimization\n",
    "spark.read.format(\"json\")\\\n",
    "    .load(\"book-data/json/2015-summary.json\")\\\n",
    "    .sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5da3713-38ba-42c3-a305-a8ba4b40e555",
   "metadata": {},
   "source": [
    "### Limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bb4bd137-7bad-42f2-9da3-5cc9d3c8e9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#limit\n",
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "035204a0-6bbb-4e4c-9848-a1d2470bd7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "|       United States|          Singapore|    1|\n",
      "|             Moldova|      United States|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a9a2f6-073c-46fd-9ee1-4ed51fe461e7",
   "metadata": {},
   "source": [
    "### Repartition and Coalesce\n",
    "\n",
    "Repartitioln: full shuffle of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "93704e17-03cd-40be-8a01-ae67349b3f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repartition and Coalesce\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "91d1048d-91a1-4816-9426-257d0b8b21e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7675689-f1fa-43f4-a60a-12320c61a918",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f5cc51ba-b087-42a6-aa22-0a0115299b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fbfc7227-bac9-4a03-8477-42ea4fb5c2d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5,col(\"DEST_COUNTRY_NAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "5950dd79-df1d-4478-b537-acfb905436c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.repartition(5,col(\"DEST_COUNTRY_NAME\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1164f7c6-3c49-4c65-b639-5f435c41aa4f",
   "metadata": {},
   "source": [
    "### Collecting Rows to the Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f5fc8e98-5102-4319-b7db-c8d02d6a0f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "|United States    |India              |62   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344),\n",
       " Row(DEST_COUNTRY_NAME='Egypt', ORIGIN_COUNTRY_NAME='United States', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='India', count=62),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62),\n",
       " Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588),\n",
       " Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Collecting Rows to the Driver\n",
    "collectDF=df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5,False)\n",
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf145ec0-7e1d-4ce2-9b57-fe2c4b50d5ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
